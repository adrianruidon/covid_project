{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, row_number,  col, when, rand\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, FloatType, NumericType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar ficheros con ciudades y países"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los datos de COVID descargados el 20200602.json han sido guardados en 'data\\20200602.json'.\n"
     ]
    }
   ],
   "source": [
    "def descargar_datos_covid(url_api, fecha):\n",
    "    response = requests.get(url_api)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        datos_json = response.json()\n",
    "        \n",
    "        nombre_archivo = f\"{fecha}.json\"\n",
    "        \n",
    "        directorio = 'data'\n",
    "        if not os.path.exists(directorio):\n",
    "            os.makedirs(directorio)\n",
    "        \n",
    "        ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "        with open(ruta_archivo, 'w') as file:\n",
    "            json.dump(datos_json, file, indent=4)\n",
    "        \n",
    "        print(f\"Los datos de COVID descargados el {fecha}.json han sido guardados en '{ruta_archivo}'.\")\n",
    "    else:\n",
    "        print(\"No se pudo descargar los datos. Estado de la solicitud:\", response.status_code)\n",
    "\n",
    "fecha = \"20200602\"\n",
    "url_api = f\"https://api.covidtracking.com/v1/us/{fecha}.json\"\n",
    "\n",
    "descargar_datos_covid(url_api, fecha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Generar CSV\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_csv(spark):\n",
    "    cities = [\n",
    "        Row(\n",
    "            id=1,\n",
    "            country_id=1,\n",
    "            name=\"Valencia\",\n",
    "            lat=\"39.47010514497293\",\n",
    "            lng=\"-0.3766213363909406\",\n",
    "        ),\n",
    "        Row(\n",
    "            id=2,\n",
    "            country_id=2,\n",
    "            name=\"París\",\n",
    "            lat=\"48.85663074697171\",\n",
    "            lng=\"2.3518663102905877\",\n",
    "        ),\n",
    "        Row(\n",
    "            id=3,\n",
    "            country_id=3,\n",
    "            name=\"New York\",\n",
    "            lat=\"40.7128\",\n",
    "            lng=\"-74.0060\",\n",
    "        ),\n",
    "        Row(\n",
    "            id=4, country_id=4, name=\"Tokyo\", lat=\"35.6895\", lng=\"139.6917\"\n",
    "        ),\n",
    "        Row(\n",
    "            id=5, country_id=5, name=\"London\", lat=\"51.5074\", lng=\"-0.1278\"\n",
    "        ),\n",
    "        Row(\n",
    "            id=6,\n",
    "            country_id=6,\n",
    "            name=\"Sydney\",\n",
    "            lat=\"-33.8743710663433\",\n",
    "            lng=\"151.0535114434631\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    countries = [\n",
    "        Row(\n",
    "            id=1,\n",
    "            name=\"España\",\n",
    "        ),\n",
    "        Row(\n",
    "            id=2,\n",
    "            name=\"Francia\",\n",
    "        ),\n",
    "        Row(\n",
    "            id=3,\n",
    "            name=\"Estados Unidos\",\n",
    "        ),\n",
    "        Row(id=4, name=\"Japón\"),\n",
    "        Row(id=5, name=\"Inglaterra\"),\n",
    "        Row(\n",
    "            id=6,\n",
    "            name=\"Australia\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    cities_df = spark.createDataFrame(\n",
    "        cities,\n",
    "        StructType(\n",
    "            [\n",
    "                StructField(\"id\", IntegerType(), True),\n",
    "                StructField(\"country_id\", IntegerType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"lat\", StringType(), True),\n",
    "                StructField(\"lng\", StringType(), True),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    countries_df = spark.createDataFrame(\n",
    "        countries,\n",
    "        StructType(\n",
    "            [\n",
    "                StructField(\"id\", IntegerType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Creación de los archivos de ciudades.csv y paises.csv\n",
    "    directorio_script = os.getcwd()\n",
    "    directorio_salida = os.path.join(directorio_script, \"datos\")\n",
    "\n",
    "    if not os.path.exists(directorio_salida):\n",
    "        os.makedirs(directorio_salida)\n",
    "\n",
    "    ruta_salida_ciudades = os.path.join(directorio_salida, \"ciudades.csv\")\n",
    "    ruta_salida_paises = os.path.join(directorio_salida, \"paises.csv\")\n",
    "\n",
    "    ciudades_pandas_df = cities_df.toPandas()\n",
    "    paises_pandas_df = countries_df.toPandas()\n",
    "\n",
    "    ciudades_pandas_df.to_csv(ruta_salida_ciudades, index=False)\n",
    "    paises_pandas_df.to_csv(ruta_salida_paises, index=False)\n",
    "\n",
    "\n",
    "generar_csv(spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_fechas(fecha: int, n_veces: int) -> List[int]:\n",
    "    año = fecha // 10000\n",
    "    mes = (fecha // 100) % 100\n",
    "    día = fecha % 100\n",
    "\n",
    "    fecha_actual = datetime(año, mes, día)\n",
    "    fechas = []\n",
    "\n",
    "    for _ in range(n_veces):\n",
    "        fecha_actual += timedelta(days=1)\n",
    "        fechas.append(int(fecha_actual.strftime('%Y%m%d')))\n",
    "            \n",
    "    return fechas\n",
    "\n",
    "def csv_a_parquet(ruta_csv: str | None = None, delimitador: str = \",\") -> bool:\n",
    "    try:\n",
    "        if ruta_csv is None: \n",
    "            raise ValueError(\"Introduce la ruta del CSV\")\n",
    "        \n",
    "        partes_ruta = ruta_csv.split(\"/\")\n",
    "        carpeta_ruta = \"/\".join(partes_ruta[:-1])\n",
    "        nuevo_nombre = partes_ruta[-1].split(\".\")[0] + \".parquet\"\n",
    "        \n",
    "        pd.read_csv(filepath_or_buffer=ruta_csv, delimiter=delimitador).to_parquet(path=f\"{carpeta_ruta}/{nuevo_nombre}\")\n",
    "        print(\"¡Transformación realizada con éxito! :)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet saved, name: covid_info.parquet :)\n",
      "Error: [Errno 2] No such file or directory: '../data/transformed_covid_data.csv'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    RUTA_CARPETA = os.getcwd()\n",
    "    ruta_covid_diario = os.path.join(RUTA_CARPETA, \"datos\", f\"{fecha}.json\")\n",
    "    df = spark.read.option(\"multiline\", True).json(ruta_covid_diario)\n",
    "    df.createOrReplaceTempView(\"json\")\n",
    "\n",
    "    columnas_numericas = [\n",
    "        col.name for col in df.schema.fields if isinstance(col.dataType, NumericType)\n",
    "    ]\n",
    "\n",
    "    esquema = StructType(\n",
    "        [\n",
    "            *[\n",
    "                StructField(nombre_col, df.schema[nombre_col].dataType, True)\n",
    "                for nombre_col in [*columnas_numericas, \"dateChecked\"]\n",
    "            ],\n",
    "            StructField(\"id_ciudad\", IntegerType(), True),\n",
    "            StructField(\"factor_multiplicacion\", FloatType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        agregar_filas = []\n",
    "        for i in range(6):\n",
    "            N_VECES = 120\n",
    "            ciudad = i\n",
    "            filas = df.select(*columnas_numericas, \"dateChecked\").collect()[0]\n",
    "            fila = filas.asDict()\n",
    "            fechas = generar_fechas(fecha=fila[\"date\"], n_veces=N_VECES)\n",
    "            a_fila = {}\n",
    "\n",
    "            for j in range(N_VECES):\n",
    "                factor_m = random.uniform(0.5, 1.6)\n",
    "                for clave in fila:\n",
    "                    match clave:\n",
    "                        case \"date\":\n",
    "                            a_fila[clave] = fechas[j]\n",
    "                        case \"dateChecked\":\n",
    "                            a_fila[clave] = fila[clave]\n",
    "                        case _:\n",
    "                            nuevo_valor = int(np.floor(fila[clave] * factor_m))\n",
    "                            a_fila[clave] = nuevo_valor\n",
    "\n",
    "                a_fila[\"id_ciudad\"] = ciudad+1\n",
    "                a_fila[\"factor_multiplicacion\"] = factor_m\n",
    "                agregar_filas.append(Row(**a_fila))\n",
    "\n",
    "        df_parquet = spark.createDataFrame(agregar_filas, esquema)\n",
    "        df_parquet.createOrReplaceTempView(\"parquet\")\n",
    "        df_parquet.toPandas().to_parquet(\"./datos/info_covid.parquet\")\n",
    "        df_parquet.toPandas().to_csv(\"./datos/info_covid.csv\")\n",
    "        print(\"Parquet guardado, nombre: info_covid.parquet :)\")\n",
    "        \n",
    "        csv_a_parquet(ruta_csv=\"../datos/datos_covid_transformados.csv\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al intentar guardar los datos en el archivo parquet: {}\".format(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
